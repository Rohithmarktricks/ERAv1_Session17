{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "from os.path import exists\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "import re\n",
    "from transformer.bert_helper import SentencesDataset, get_batch\n",
    "from transformer.transformer import BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing..\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# #Init\n",
    "# =============================================================================\n",
    "print('initializing..')\n",
    "batch_size = 1024\n",
    "seq_len = 20\n",
    "embed_size = 128\n",
    "inner_ff_size = embed_size * 4\n",
    "n_heads = 8\n",
    "n_code = 8\n",
    "n_vocab = 40000\n",
    "dropout = 0.1\n",
    "# n_workers = 12\n",
    "\n",
    "#optimizer\n",
    "optim_kwargs = {'lr':1e-4, 'weight_decay':1e-4, 'betas':(.9,.999)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading text...\n",
      "tokenizing sentences...\n",
      "creating/loading vocab...\n",
      "creating dataset...\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Input\n",
    "# =============================================================================\n",
    "#1) load text\n",
    "print('loading text...')\n",
    "pth = 'Bert/training.txt'\n",
    "sentences = open(pth).read().lower().split('\\n')\n",
    "\n",
    "#2) tokenize sentences (can be done during training, you can also use spacy udpipe)\n",
    "print('tokenizing sentences...')\n",
    "special_chars = ',?;.:/*!+-()[]{}\"\\'&'\n",
    "sentences = [re.sub(f'[{re.escape(special_chars)}]', ' \\g<0> ', s).split(' ') for s in sentences]\n",
    "sentences = [[w for w in s if len(w)] for s in sentences]\n",
    "\n",
    "#3) create vocab if not already created\n",
    "print('creating/loading vocab...')\n",
    "pth = 'vocab.txt'\n",
    "if not exists(pth):\n",
    "    words = [w for s in sentences for w in s]\n",
    "    vocab = Counter(words).most_common(n_vocab) #keep the N most frequent words\n",
    "    vocab = [w[0] for w in vocab]\n",
    "    open(pth, 'w+').write('\\n'.join(vocab))\n",
    "else:\n",
    "    vocab = open(pth).read().split('\\n')\n",
    "\n",
    "#4) create dataset\n",
    "print('creating dataset...')\n",
    "dataset = SentencesDataset(sentences, vocab, seq_len)\n",
    "# kwargs = {'num_workers':n_workers, 'shuffle':True,  'drop_last':True, 'pin_memory':True, 'batch_size':batch_size}\n",
    "kwargs = {'shuffle':True,  'drop_last':True, 'pin_memory':True, 'batch_size':batch_size}\n",
    "data_loader = torch.utils.data.DataLoader(dataset, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing model...\n",
      "initializing optimizer and loss...\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Model\n",
    "# =============================================================================\n",
    "#init model\n",
    "print('initializing model...')\n",
    "model = BERT(n_code, n_heads, embed_size, inner_ff_size, len(dataset.vocab), seq_len, dropout)\n",
    "model = model.cuda()\n",
    "\n",
    "# =============================================================================\n",
    "# Optimizer\n",
    "# =============================================================================\n",
    "print('initializing optimizer and loss...')\n",
    "optimizer = optim.Adam(model.parameters(), **optim_kwargs)\n",
    "loss_model = nn.CrossEntropyLoss(ignore_index=dataset.IGNORE_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...\n",
      "it: 0  | loss 10.32  | Δw: 1.164\n",
      "it: 10  | loss 9.61  | Δw: 0.594\n",
      "it: 20  | loss 9.39  | Δw: 0.351\n",
      "it: 30  | loss 9.21  | Δw: 0.25\n",
      "it: 40  | loss 8.99  | Δw: 0.21\n",
      "it: 50  | loss 8.84  | Δw: 0.179\n",
      "it: 60  | loss 8.67  | Δw: 0.162\n",
      "it: 70  | loss 8.53  | Δw: 0.15\n",
      "it: 80  | loss 8.36  | Δw: 0.138\n",
      "it: 90  | loss 8.21  | Δw: 0.128\n",
      "it: 100  | loss 8.04  | Δw: 0.116\n",
      "it: 110  | loss 7.9  | Δw: 0.112\n",
      "it: 120  | loss 7.77  | Δw: 0.104\n",
      "it: 130  | loss 7.66  | Δw: 0.099\n",
      "it: 140  | loss 7.53  | Δw: 0.095\n",
      "it: 150  | loss 7.42  | Δw: 0.092\n",
      "it: 160  | loss 7.27  | Δw: 0.088\n",
      "it: 170  | loss 7.16  | Δw: 0.085\n",
      "it: 180  | loss 7.08  | Δw: 0.08\n",
      "it: 190  | loss 7.03  | Δw: 0.081\n",
      "it: 200  | loss 6.94  | Δw: 0.079\n",
      "it: 210  | loss 6.85  | Δw: 0.072\n",
      "it: 220  | loss 6.73  | Δw: 0.073\n",
      "it: 230  | loss 6.7  | Δw: 0.071\n",
      "it: 240  | loss 6.72  | Δw: 0.069\n",
      "it: 250  | loss 6.65  | Δw: 0.07\n",
      "it: 260  | loss 6.66  | Δw: 0.07\n",
      "it: 270  | loss 6.58  | Δw: 0.068\n",
      "it: 280  | loss 6.57  | Δw: 0.067\n",
      "it: 290  | loss 6.53  | Δw: 0.07\n",
      "it: 300  | loss 6.49  | Δw: 0.068\n",
      "it: 310  | loss 6.55  | Δw: 0.069\n",
      "it: 320  | loss 6.43  | Δw: 0.069\n",
      "it: 330  | loss 6.42  | Δw: 0.073\n",
      "it: 340  | loss 6.45  | Δw: 0.07\n",
      "it: 350  | loss 6.41  | Δw: 0.071\n",
      "it: 360  | loss 6.41  | Δw: 0.069\n",
      "it: 370  | loss 6.43  | Δw: 0.074\n",
      "it: 380  | loss 6.4  | Δw: 0.075\n",
      "it: 390  | loss 6.4  | Δw: 0.075\n",
      "it: 400  | loss 6.36  | Δw: 0.076\n",
      "it: 410  | loss 6.37  | Δw: 0.08\n",
      "it: 420  | loss 6.35  | Δw: 0.085\n",
      "it: 430  | loss 6.44  | Δw: 0.086\n",
      "it: 440  | loss 6.34  | Δw: 0.091\n",
      "it: 450  | loss 6.45  | Δw: 0.092\n",
      "it: 460  | loss 6.31  | Δw: 0.098\n",
      "it: 470  | loss 6.4  | Δw: 0.093\n",
      "it: 480  | loss 6.33  | Δw: 0.128\n",
      "it: 490  | loss 6.28  | Δw: 0.106\n",
      "it: 500  | loss 6.33  | Δw: 0.11\n",
      "it: 510  | loss 6.41  | Δw: 0.142\n",
      "it: 520  | loss 6.32  | Δw: 0.122\n",
      "it: 530  | loss 6.38  | Δw: 0.133\n",
      "it: 540  | loss 6.36  | Δw: 0.139\n",
      "it: 550  | loss 6.31  | Δw: 0.139\n",
      "it: 560  | loss 6.32  | Δw: 0.152\n",
      "it: 570  | loss 6.42  | Δw: 0.164\n",
      "it: 580  | loss 6.4  | Δw: 0.163\n",
      "it: 590  | loss 6.26  | Δw: 0.199\n",
      "it: 600  | loss 6.33  | Δw: 0.183\n",
      "it: 610  | loss 6.32  | Δw: 0.189\n",
      "it: 620  | loss 6.39  | Δw: 0.205\n",
      "it: 630  | loss 6.33  | Δw: 0.223\n",
      "it: 640  | loss 6.39  | Δw: 0.258\n",
      "it: 650  | loss 6.34  | Δw: 0.231\n",
      "it: 660  | loss 6.36  | Δw: 0.247\n",
      "it: 670  | loss 6.31  | Δw: 0.273\n",
      "it: 680  | loss 6.42  | Δw: 0.282\n",
      "it: 690  | loss 6.27  | Δw: 0.322\n",
      "it: 700  | loss 6.3  | Δw: 0.335\n",
      "it: 710  | loss 6.31  | Δw: 0.351\n",
      "it: 720  | loss 6.23  | Δw: 0.37\n",
      "it: 730  | loss 6.32  | Δw: 0.371\n",
      "it: 740  | loss 6.33  | Δw: 0.371\n",
      "it: 750  | loss 6.27  | Δw: 0.427\n",
      "it: 760  | loss 6.23  | Δw: 0.432\n",
      "it: 770  | loss 6.24  | Δw: 0.441\n",
      "it: 780  | loss 6.28  | Δw: 0.429\n",
      "it: 790  | loss 6.26  | Δw: 0.485\n",
      "it: 800  | loss 6.28  | Δw: 0.518\n",
      "it: 810  | loss 6.36  | Δw: 0.523\n",
      "it: 820  | loss 6.27  | Δw: 0.496\n",
      "it: 830  | loss 6.2  | Δw: 0.594\n",
      "it: 840  | loss 6.3  | Δw: 0.567\n",
      "it: 850  | loss 6.27  | Δw: 0.582\n",
      "it: 860  | loss 6.22  | Δw: 0.601\n",
      "it: 870  | loss 6.25  | Δw: 0.615\n",
      "it: 880  | loss 6.21  | Δw: 0.688\n",
      "it: 890  | loss 6.27  | Δw: 0.649\n",
      "it: 900  | loss 6.28  | Δw: 0.656\n",
      "it: 910  | loss 6.26  | Δw: 0.711\n",
      "it: 920  | loss 6.3  | Δw: 0.66\n",
      "it: 930  | loss 6.24  | Δw: 0.787\n",
      "it: 940  | loss 6.22  | Δw: 0.686\n",
      "it: 950  | loss 6.22  | Δw: 0.726\n",
      "it: 960  | loss 6.27  | Δw: 0.738\n",
      "it: 970  | loss 6.16  | Δw: 0.736\n",
      "it: 980  | loss 6.28  | Δw: 0.831\n",
      "it: 990  | loss 6.25  | Δw: 0.819\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "# =============================================================================\n",
    "print('training...')\n",
    "print_each = 10\n",
    "model.train()\n",
    "batch_iter = iter(data_loader)\n",
    "n_iteration = 1000\n",
    "for it in range(n_iteration):\n",
    "\n",
    "    #get batch\n",
    "    batch, batch_iter = get_batch(data_loader, batch_iter)\n",
    "\n",
    "    #infer\n",
    "    masked_input = batch['input']\n",
    "    masked_target = batch['target']\n",
    "\n",
    "    masked_input = masked_input.cuda(non_blocking=True)\n",
    "    masked_target = masked_target.cuda(non_blocking=True)\n",
    "    output = model(masked_input)\n",
    "\n",
    "    #compute the cross entropy loss\n",
    "    output_v = output.view(-1,output.shape[-1])\n",
    "    target_v = masked_target.view(-1,1).squeeze()\n",
    "    loss = loss_model(output_v, target_v)\n",
    "\n",
    "    #compute gradients\n",
    "    loss.backward()\n",
    "\n",
    "    #apply gradients\n",
    "    optimizer.step()\n",
    "\n",
    "    #print step\n",
    "    if it % print_each == 0:\n",
    "        print('it:', it,\n",
    "              ' | loss', np.round(loss.item(),2),\n",
    "              ' | Δw:', round(model.embeddings.weight.grad.abs().sum().item(),3))\n",
    "\n",
    "    #reset gradients\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving embeddings...\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Results analysis\n",
    "# =============================================================================\n",
    "print('saving embeddings...')\n",
    "N = 3000\n",
    "np.savetxt('values.tsv', np.round(model.embeddings.weight.detach().cpu().numpy()[0:N], 2), delimiter='\\t', fmt='%1.2f')\n",
    "s = [dataset.rvocab[i] for i in range(N)]\n",
    "open('names.tsv', 'w+').write('\\n'.join(s) )\n",
    "\n",
    "\n",
    "print('end')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
